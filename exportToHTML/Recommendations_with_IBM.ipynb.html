<html>
<head>
<title>Recommendations_with_IBM.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
Recommendations_with_IBM.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1"># Recommendation System Project: IBM Community 
 
In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform.  
 
 
You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/3325/view).  **Please save regularly.** 
 
By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations.  
 
 
## Table of Contents 
 
I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)&lt;br&gt; 
II. [Rank Based Recommendations](#Rank)&lt;br&gt; 
III. [User-User Based Collaborative Filtering](#User-User)&lt;br&gt; 
IV. [Content Based Recommendations](#Content-Recs)&lt;br&gt; 
V. [Matrix Factorization](#Matrix-Fact)&lt;br&gt; 
VI. [Extras &amp; Concluding](#conclusions) 
 
At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data. 
</span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">pandas</span>
<span class="s0"># libraries</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s2">import </span><span class="s1">project_tests </span><span class="s2">as </span><span class="s1">t</span>
<span class="s2">import </span><span class="s1">seaborn </span><span class="s2">as </span><span class="s1">sns</span>

<span class="s0">#%% 
# load data</span>
<span class="s1">df = pd.read_csv(</span>
    <span class="s3">'data/user-item-interactions.csv'</span><span class="s2">, </span>
    <span class="s1">dtype={</span><span class="s3">'article_id'</span><span class="s1">: int</span><span class="s2">, </span><span class="s3">'title'</span><span class="s1">: str</span><span class="s2">, </span><span class="s3">'email'</span><span class="s1">: str}</span>
<span class="s1">)</span>
<span class="s0"># Show df to get an idea of the data</span>
<span class="s1">df.head()</span>
<span class="s0">#%% md 
</span><span class="s1">### &lt;a class=&quot;anchor&quot; id=&quot;Exploratory-Data-Analysis&quot;&gt;Part I : Exploratory Data Analysis&lt;/a&gt; 
 
Use the dictionary and cells below to provide some insight into the descriptive statistics of the data. 
 
`1.` Are there any missing values? If so, provide a count of missing values. If there are missing values in `email`, assign it the same id value `&quot;unknown_user&quot;`. 
</span><span class="s0">#%% 
# Some interactions do not have a user associated with it, assume the same user.</span>
<span class="s1">df.info()</span>
<span class="s0">#%% 
</span><span class="s1">print(</span><span class="s3">f&quot;Number of Null email values is: &quot;</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">df[df.email.isna()]</span>
<span class="s0">#%% 
# Fill email NaNs with &quot;unknown_user&quot;</span>
<span class="s1">df.loc[df[</span><span class="s3">'email'</span><span class="s1">].isna()</span><span class="s2">, </span><span class="s3">'email'</span><span class="s1">] = </span><span class="s3">'unknown_user'</span>
<span class="s0">#%% 
# Check if no more NaNs </span>
<span class="s1">df[df.email.isna()]</span>
<span class="s0">#%% md 
</span><span class="s1">`2.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article. 
</span><span class="s0">#%% 
# What are the descriptive statistics of the number of articles a user interacts with?</span>
<span class="s1">df_user_articles = df.groupby(</span><span class="s3">'email'</span><span class="s1">)[</span><span class="s3">'article_id'</span><span class="s1">].count().reset_index()</span>
<span class="s1">df_user_articles.columns = [</span><span class="s3">'email'</span><span class="s2">, </span><span class="s3">'article_count'</span><span class="s1">]</span>
<span class="s1">df_user_articles[</span><span class="s3">&quot;article_count&quot;</span><span class="s1">].describe()</span>
<span class="s0">#%% 
# Create a plot of the number of articles read by each user</span>
<span class="s1">sns.barplot(data=df_user_articles</span><span class="s2">, </span><span class="s1">x=</span><span class="s3">'article_count'</span><span class="s2">, </span><span class="s1">y=</span><span class="s3">'email'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'number of articles'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'number of users'</span><span class="s1">)</span>
<span class="s1">plt.yticks([])</span>
<span class="s1">plt.title(</span><span class="s3">'Number of Users Reading Articles'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
</span><span class="s1">article_counts = df.groupby(</span><span class="s3">'article_id'</span><span class="s1">)[</span><span class="s3">'email'</span><span class="s1">].count().reset_index()</span>
<span class="s1">article_counts.columns = [</span><span class="s3">'article_id'</span><span class="s2">, </span><span class="s3">'read_count'</span><span class="s1">]</span>
<span class="s1">article_counts[</span><span class="s3">'read_count'</span><span class="s1">].describe()</span>
<span class="s0">#%% 
# Create a plot of the number of times each article was read</span>

<span class="s1">sns.barplot(data=article_counts</span><span class="s2">, </span><span class="s1">y=</span><span class="s3">'read_count'</span><span class="s2">, </span><span class="s1">x=</span><span class="s3">'article_id'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'number of users'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'number of articles'</span><span class="s1">)</span>
<span class="s1">plt.xticks([])</span>
<span class="s1">plt.title(</span><span class="s3">'Distribution of Article Usage'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
# Fill in the median and maximum number of user_article interactions below</span>

<span class="s1">median_val = np.median(df_user_articles[</span><span class="s3">&quot;article_count&quot;</span><span class="s1">]) </span><span class="s0"># 50% of individuals interact with 3 number of articles or fewer.</span>
<span class="s1">max_views_by_user =  np.max(df_user_articles[</span><span class="s3">&quot;article_count&quot;</span><span class="s1">])</span><span class="s0"># The maximum number of user-article interactions by any 1 user is 364.</span>
<span class="s0">#%% md 
</span><span class="s1">`3.` Use the cells below to find: 
 
**a.** The number of unique articles that have an interaction with a user.   
**b.** The number of unique articles in the dataset (whether they have any interactions or not).&lt;br&gt; 
**c.** The number of unique users in the dataset. (excluding null values) &lt;br&gt; 
**d.** The number of user-article interactions in the dataset. 
</span><span class="s0">#%% 
</span><span class="s1">unique_articles = len(article_counts[article_counts[</span><span class="s3">&quot;read_count&quot;</span><span class="s1">] &gt;= </span><span class="s4">1</span><span class="s1">])</span>
 <span class="s0"># The number of unique articles that have at least one interaction</span>
<span class="s1">total_articles = len(df[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].unique()) </span><span class="s0"># The number of unique articles on the IBM platform</span>
<span class="s1">unique_users =  len(df[</span><span class="s3">&quot;email&quot;</span><span class="s1">].unique()) </span><span class="s0"># The number of unique users</span>
<span class="s1">user_article_interactions = sum(article_counts[</span><span class="s3">&quot;read_count&quot;</span><span class="s1">]) </span><span class="s0"># The number of user-article interactions</span>
<span class="s0">#%% md 
</span><span class="s1">`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below). 
</span><span class="s0">#%% 
</span><span class="s1">most_viewed_article_id =  article_counts.loc[article_counts[</span><span class="s3">&quot;read_count&quot;</span><span class="s1">].idxmax()</span><span class="s2">, </span><span class="s3">&quot;article_id&quot;</span><span class="s1">]</span><span class="s0"># The most viewed article in the dataset as a string with one value following the decimal </span>
<span class="s1">max_views =  article_counts[</span><span class="s3">&quot;read_count&quot;</span><span class="s1">].max()</span><span class="s0"># The most viewed article in the dataset was viewed how many times?</span>
<span class="s0">#%% 
## No need to change the code here - this will be helpful for later parts of the notebook</span>
<span class="s0"># Run this cell to map the user email to a user_id column and remove the email column</span>

<span class="s2">def </span><span class="s1">email_mapper(df=df):</span>
    <span class="s1">coded_dict = {</span>
        <span class="s1">email: num </span>
        <span class="s2">for </span><span class="s1">num</span><span class="s2">, </span><span class="s1">email </span><span class="s2">in </span><span class="s1">enumerate(df[</span><span class="s3">'email'</span><span class="s1">].unique()</span><span class="s2">, </span><span class="s1">start=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">}</span>
    <span class="s2">return </span><span class="s1">[coded_dict[val] </span><span class="s2">for </span><span class="s1">val </span><span class="s2">in </span><span class="s1">df[</span><span class="s3">'email'</span><span class="s1">]]</span>

<span class="s1">df[</span><span class="s3">'user_id'</span><span class="s1">] = email_mapper(df)</span>
<span class="s2">del </span><span class="s1">df[</span><span class="s3">'email'</span><span class="s1">]</span>

<span class="s0"># show header</span>
<span class="s1">df.head()</span>
<span class="s0">#%% 
## If you stored all your results in the variable names above, </span>
<span class="s0">## you shouldn't need to change anything in this cell</span>

<span class="s1">sol_1_dict = {</span>
    <span class="s3">'`50% of individuals have _____ or fewer interactions.`'</span><span class="s1">: median_val</span><span class="s2">,</span>
    <span class="s3">'`The total number of user-article interactions in the dataset is ______.`'</span><span class="s1">: user_article_interactions</span><span class="s2">,</span>
    <span class="s3">'`The maximum number of user-article interactions by any 1 user is ______.`'</span><span class="s1">: max_views_by_user</span><span class="s2">,</span>
    <span class="s3">'`The most viewed article in the dataset was viewed _____ times.`'</span><span class="s1">: max_views</span><span class="s2">,</span>
    <span class="s3">'`The article_id of the most viewed article is ______.`'</span><span class="s1">: most_viewed_article_id</span><span class="s2">,</span>
    <span class="s3">'`The number of unique articles that have at least 1 rating ______.`'</span><span class="s1">: unique_articles</span><span class="s2">,</span>
    <span class="s3">'`The number of unique users in the dataset is ______`'</span><span class="s1">: unique_users</span><span class="s2">,</span>
    <span class="s3">'`The number of unique articles on the IBM platform`'</span><span class="s1">: total_articles</span>
<span class="s1">}</span>

<span class="s0"># Test your dictionary against the solution</span>
<span class="s1">t.sol_1_test(sol_1_dict)</span>
<span class="s0">#%% md 
</span><span class="s1">### &lt;a class=&quot;anchor&quot; id=&quot;Rank&quot;&gt;Part II: Rank-Based Recommendations&lt;/a&gt; 
 
In this project, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article. In these cases, the popularity of an article can really only be based on how often an article was interacted with. 
 
`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below. 
</span><span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">get_top_articles(n</span><span class="s2">, </span><span class="s1">df=df):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    n - (int) the number of top articles to return 
    df - (pandas dataframe) df as defined at the top of the notebook  
     
    OUTPUT: 
    top_articles - (list) A list of the top 'n' article titles  
     
    &quot;&quot;&quot;</span>
    <span class="s1">top_articles = df.groupby(</span><span class="s3">'article_id'</span><span class="s1">)[</span><span class="s3">'user_id'</span><span class="s1">].count().reset_index()</span>
    <span class="s1">top_articles = top_articles.sort_values(by=</span><span class="s3">&quot;user_id&quot;</span><span class="s2">, </span><span class="s1">ascending= </span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">top_articles = top_articles[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].iloc[</span><span class="s4">0</span><span class="s1">:n]</span>
    <span class="s1">top_articles = df[df[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].isin(top_articles)][</span><span class="s3">&quot;title&quot;</span><span class="s1">].unique().tolist()</span>
    
    <span class="s2">return </span><span class="s1">top_articles </span><span class="s0"># Return the top article titles from df</span>

<span class="s2">def </span><span class="s1">get_top_article_ids(n</span><span class="s2">, </span><span class="s1">df=df):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    n - (int) the number of top articles to return 
    df - (pandas dataframe) df as defined at the top of the notebook  
     
    OUTPUT: 
    top_articles - (list) A list of the top 'n' article ids  
     
    &quot;&quot;&quot;</span>
    <span class="s1">top_articles = df.groupby(</span><span class="s3">'article_id'</span><span class="s1">)[</span><span class="s3">'user_id'</span><span class="s1">].count().reset_index()</span>
    <span class="s1">top_articles = top_articles.sort_values(by=</span><span class="s3">&quot;user_id&quot;</span><span class="s2">, </span><span class="s1">ascending= </span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">top_articles = top_articles[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].iloc[</span><span class="s4">0</span><span class="s1">:n].tolist()</span>
 
    <span class="s2">return </span><span class="s1">top_articles </span><span class="s0"># Return the top article ids</span>
<span class="s0">#%% 
</span><span class="s1">print(get_top_articles(</span><span class="s4">10</span><span class="s1">))</span>
<span class="s1">print(get_top_article_ids(</span><span class="s4">10</span><span class="s1">))</span>
<span class="s0">#%% 
# Test your function by returning the top 5, 10, and 20 articles</span>
<span class="s1">top_5 = get_top_articles(</span><span class="s4">5</span><span class="s1">)</span>
<span class="s1">top_10 = get_top_articles(</span><span class="s4">10</span><span class="s1">)</span>
<span class="s1">top_20 = get_top_articles(</span><span class="s4">20</span><span class="s1">)</span>

<span class="s0"># Test each of your three lists from above</span>
<span class="s1">t.sol_2_test(get_top_articles)</span>
<span class="s0">#%% md 
</span><span class="s1">### &lt;a class=&quot;anchor&quot; id=&quot;User-User&quot;&gt;Part III: User-User Based Collaborative Filtering&lt;/a&gt; 
 
 
`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.   
 
* Each **user** should only appear in each **row** once. 
 
 
* Each **article** should only show up in one **column**.   
 
 
* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.   
 
 
* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**.  
 
Use the tests to make sure the basic structure of your matrix matches what is expected by the solution. 
</span><span class="s0">#%% 
# create the user-article matrix with 1's and 0's</span>

<span class="s2">def </span><span class="s1">create_user_item_matrix(df</span><span class="s2">, </span><span class="s1">fill_value=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    df - pandas dataframe with article_id, title, user_id columns 
     
    OUTPUT: 
    user_item - user item matrix  
     
    Description: 
    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with  
    an article and a 0 otherwise 
    &quot;&quot;&quot;</span>
    
    <span class="s0"># convert to string to ensure correct column renaming</span>
   
    <span class="s1">data = df.copy()</span>
    <span class="s1">data[</span><span class="s3">'article_id'</span><span class="s1">] = data[</span><span class="s3">'article_id'</span><span class="s1">].astype(str)</span>
    
    <span class="s1">unique_article_ids = data[</span><span class="s3">'article_id'</span><span class="s1">].unique()</span>

    <span class="s0"># pivot data</span>
    <span class="s1">user_item = data[data[</span><span class="s3">'article_id'</span><span class="s1">].isin(unique_article_ids)].pivot_table(</span>
        <span class="s1">index=</span><span class="s3">'user_id'</span><span class="s2">, </span>
        <span class="s1">columns=</span><span class="s3">'article_id'</span><span class="s2">, </span>
        <span class="s1">aggfunc=</span><span class="s3">'size'</span><span class="s2">,  </span><span class="s0"># Count occurrences of interactions</span>
        <span class="s1">fill_value=</span><span class="s4">0</span>
    <span class="s1">)</span>

    <span class="s0"># ensure that pivoted table is binary </span>
    <span class="s1">user_item[user_item &gt;= </span><span class="s4">1</span><span class="s1">] = </span><span class="s4">1</span>
    
    <span class="s2">return </span><span class="s1">user_item </span><span class="s0"># return the user_item matrix </span>

<span class="s1">user_item = create_user_item_matrix(df)</span>
<span class="s0">#%% 
## Tests: You should just need to run this cell.  Don't change the code.</span>
<span class="s2">assert </span><span class="s1">user_item.shape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">5149</span><span class="s2">, </span><span class="s3">&quot;Oops!  The number of users in the user-article matrix doesn't look right.&quot;</span>
<span class="s2">assert </span><span class="s1">user_item.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">714</span><span class="s2">, </span><span class="s3">&quot;Oops!  The number of articles in the user-article matrix doesn't look right.&quot;</span>
<span class="s2">assert </span><span class="s1">user_item.sum(axis=</span><span class="s4">1</span><span class="s1">)[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">36</span><span class="s2">, </span><span class="s3">&quot;Oops!  The number of articles seen by user 1 doesn't look right.&quot;</span>
<span class="s1">print(</span><span class="s3">&quot;You have passed our quick tests!  Please proceed!&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users.  
 
Use the tests to test your function. 
</span><span class="s0">#%% 
# Lets use the cosine_similarity function from sklearn</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s2">import </span><span class="s1">cosine_similarity</span>
<span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">find_similar_users(user_id</span><span class="s2">, </span><span class="s1">user_item=user_item</span><span class="s2">, </span><span class="s1">include_similarity=</span><span class="s2">False</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    user_id - (int) a user_id 
    user_item - (pandas dataframe) matrix of users by articles:  
                1's when a user has interacted with an article, 0 otherwise 
    include_similarity - (bool) whether to include the similarity in the output 
     
    OUTPUT: 
    similar_users - (list) an ordered list where the closest users (largest dot product users) 
                    are listed first 
     
    Description: 
    Computes the similarity of every pair of users based on the dot product 
    Returns an ordered list of user ids. If include_similarity is True, returns a list of lists 
    where the first element is the user id and the second the similarity. 
     
    &quot;&quot;&quot;</span>
    
    <span class="s0"># initialize user to be tested</span>
    <span class="s1">user_id -=</span><span class="s4">1 </span><span class="s0"># correct for 0 indexing in python </span>
    <span class="s1">user1 = user_item.iloc[user_id].values.reshape(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">row_ids = range(len(user_item))</span>
    <span class="s1">user_ids = range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">len(user_item)+</span><span class="s4">1</span><span class="s1">) </span><span class="s0"># user_ids start with 1</span>
    <span class="s1">similarity = []</span>
    
    <span class="s0"># iterate across user rows and calculate cosine similarity</span>
    <span class="s2">for </span><span class="s1">row </span><span class="s2">in </span><span class="s1">row_ids:</span>
        <span class="s1">user2 = user_item.iloc[row].values.reshape(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">sim = cosine_similarity(user1</span><span class="s2">, </span><span class="s1">user2)</span>
        <span class="s1">similarity.append(sim[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s0"># store similarity next to user_id</span>
    <span class="s1">sim_df = pd.DataFrame({</span><span class="s3">&quot;similarity&quot;</span><span class="s1">: similarity</span><span class="s2">,</span>
                           <span class="s3">&quot;user_id&quot;</span><span class="s1">: user_ids})</span>
    <span class="s0"># remove the own user's id</span>
    <span class="s1">sim_df = sim_df.drop(user_id)</span>
    
    <span class="s0"># sort by similarity</span>
    <span class="s1">sim_df = sim_df.sort_values(by=[</span><span class="s3">&quot;similarity&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">ascending= </span><span class="s2">False</span><span class="s1">)</span>
    
    <span class="s0"># create list of just the ids</span>
    <span class="s1">ids = sim_df[</span><span class="s3">&quot;user_id&quot;</span><span class="s1">].tolist()</span>
    <span class="s0"># create list of just the similarities</span>
    <span class="s1">sim = sim_df[</span><span class="s3">&quot;similarity&quot;</span><span class="s1">].tolist()</span>
    
    <span class="s2">if </span><span class="s1">include_similarity:</span>
        <span class="s2">return  </span><span class="s1">ids</span><span class="s2">, </span><span class="s1">sim</span>
    <span class="s2">return </span><span class="s1">ids </span><span class="s0"># return a list of the users in order from most to least similar</span>
        
<span class="s0">#%% 
# Do a spot check of your function</span>
<span class="s1">print(</span><span class="s3">&quot;The 10 most similar users to user 1 are: {}&quot;</span><span class="s1">.format(find_similar_users(</span><span class="s4">1</span><span class="s1">)[:</span><span class="s4">10</span><span class="s1">]))</span>
<span class="s1">print(</span><span class="s3">&quot;The 5 most similar users to user 3933 are: {}&quot;</span><span class="s1">.format(find_similar_users(</span><span class="s4">3933</span><span class="s1">)[:</span><span class="s4">5</span><span class="s1">]))</span>
<span class="s1">print(</span><span class="s3">&quot;The 3 most similar users to user 46 are: {}&quot;</span><span class="s1">.format(find_similar_users(</span><span class="s4">46</span><span class="s1">)[:</span><span class="s4">3</span><span class="s1">]))</span>
<span class="s0">#%% md 
</span><span class="s1">`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user.  
</span><span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">get_article_names(article_ids</span><span class="s2">, </span><span class="s1">df=df):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    article_ids - (list) a list of article ids 
    df - (pandas dataframe) df as defined at the top of the notebook 
     
    OUTPUT: 
    article_names - (list) a list of article names associated with the list of article ids  
                    (this is identified by the title column in df) 
    &quot;&quot;&quot;</span>
    
    <span class="s1">article_names = df.loc[df[</span><span class="s3">'article_id'</span><span class="s1">].isin(article_ids)</span><span class="s2">, </span><span class="s3">&quot;title&quot;</span><span class="s1">].unique()</span>
    
    
    <span class="s2">return </span><span class="s1">article_names </span><span class="s0"># Return the article names associated with list of article ids</span>

<span class="s2">def </span><span class="s1">get_ranked_article_unique_counts(article_ids</span><span class="s2">, </span><span class="s1">user_item=user_item):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    user_id - (int) a user id 
    user_item - (pandas dataframe) matrix of users by articles:  
                1's when a user has interacted with an article, 0 otherwise  
     
    OUTPUT: 
    article_counts - (list) a list of tuples with article_id and number of  
                     unique users that have interacted with the article, sorted 
                     by the number of unique users in descending order 
     
    Description: 
    Provides a list of the article_ids and the number of unique users that have 
    interacted with the article using the user_item matrix, sorted by the number 
    of unique users in descending order 
    &quot;&quot;&quot;</span>
    <span class="s1">article_ids_str = list(map(str</span><span class="s2">, </span><span class="s1">article_ids))</span>
    <span class="s1">rauc = pd.DataFrame({</span><span class="s3">&quot;article_id&quot;</span><span class="s1">: article_ids</span><span class="s2">,</span>
                     <span class="s3">&quot;interactions&quot;</span><span class="s1">: user_item[article_ids_str].sum()})</span>
    <span class="s1">rauc = rauc.sort_values(by=</span><span class="s3">&quot;interactions&quot;</span><span class="s2">, </span><span class="s1">ascending= </span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">ranked_article_unique_counts = rauc[[</span><span class="s3">&quot;article_id&quot;</span><span class="s2">, </span><span class="s3">&quot;interactions&quot;</span><span class="s1">]].values.tolist()</span>
    
    <span class="s2">return </span><span class="s1">ranked_article_unique_counts</span>


<span class="s2">def </span><span class="s1">get_user_articles(user_id</span><span class="s2">, </span><span class="s1">user_item=user_item):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    user_id - (int) a user id 
    user_item - (pandas dataframe) matrix of users by articles:  
                1's when a user has interacted with an article, 0 otherwise 
     
    OUTPUT: 
    article_ids - (list) a list of the article ids seen by the user 
    article_names - (list) a list of article names associated with the list of article ids  
                    (this is identified by the title column in df) 
     
    Description: 
    Provides a list of the article_ids and article titles that have been seen by a user 
    &quot;&quot;&quot;</span>
    
    <span class="s1">user = user_item.iloc[user_id-</span><span class="s4">1</span><span class="s1">]</span>

    <span class="s1">article_ids = user[user == </span><span class="s4">1</span><span class="s1">].index.tolist()</span>
    
    <span class="s1">article_ids = list(map(int</span><span class="s2">, </span><span class="s1">article_ids))</span>

    <span class="s1">article_names = get_article_names(article_ids)</span>
    
    
    <span class="s2">return </span><span class="s1">article_ids</span><span class="s2">, </span><span class="s1">article_names </span><span class="s0"># return the ids and names</span>


<span class="s2">def </span><span class="s1">user_user_recs(user_id</span><span class="s2">, </span><span class="s1">m=</span><span class="s4">10</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    user_id - (int) a user id 
    m - (int) the number of recommendations you want for the user 
     
    OUTPUT: 
    recs - (list) a list of recommendations for the user 
     
    Description: 
    Loops through the users based on closeness to the input user_id 
    For each user - finds articles the user hasn't seen before and provides them as recs 
    Does this until m recommendations are found 
     
    Notes: 
    Users who are the same closeness are chosen arbitrarily as the 'next' user 
     
    For the user where the number of recommended articles starts below m  
    and ends exceeding m, the last items are chosen arbitrarily 
     
    &quot;&quot;&quot;</span>
    
    <span class="s1">similar_users</span><span class="s2">, </span><span class="s1">similar_score = find_similar_users(user_id</span><span class="s2">, </span><span class="s1">include_similarity=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">recs = [] </span><span class="s0"># initalize recs</span>
    <span class="s1">user = user_item.iloc[user_id] </span><span class="s0"># initialize user</span>

    <span class="s2">for </span><span class="s1">u </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s2">,</span><span class="s1">len(similar_users)):</span>
        <span class="s1">id = similar_users[u]</span>
        <span class="s1">sim_user = user_item.iloc[id]</span>
        <span class="s1">result = user_item.columns[(sim_user == </span><span class="s4">1</span><span class="s1">) &amp; (user == </span><span class="s4">0</span><span class="s1">)].tolist()</span>
        
        <span class="s1">recs.extend([item </span><span class="s2">for </span><span class="s1">item </span><span class="s2">in </span><span class="s1">result </span><span class="s2">if </span><span class="s1">item </span><span class="s2">not in </span><span class="s1">recs])</span>
        
        <span class="s0"># Check if we have enough recommendations</span>
        <span class="s1">n_recs = len(recs)</span>
        <span class="s2">if </span><span class="s1">n_recs &gt;= m:</span>
            <span class="s2">break  </span><span class="s0"># Exit loop if we have enough recommendations</span>
    
        <span class="s1">recs = recs[:m]</span>
    
    <span class="s1">recs = list(map(int</span><span class="s2">, </span><span class="s1">recs))  </span>
    
    <span class="s2">return </span><span class="s1">recs </span><span class="s0"># return your recommendations for this user_id    </span>
<span class="s0">#%% 
# Check Results</span>
<span class="s1">get_article_names(user_user_recs(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)) </span><span class="s0"># Return 10 recommendations for user 1</span>
<span class="s0">#%% 
</span><span class="s1">get_ranked_article_unique_counts([</span><span class="s4">1320</span><span class="s2">, </span><span class="s4">232</span><span class="s2">, </span><span class="s4">844</span><span class="s1">])</span>
<span class="s0">#%% 
</span><span class="s1">get_article_names([</span><span class="s4">1024</span><span class="s2">, </span><span class="s4">1176</span><span class="s2">, </span><span class="s4">1305</span><span class="s2">, </span><span class="s4">1314</span><span class="s2">, </span><span class="s4">1422</span><span class="s2">, </span><span class="s4">1427</span><span class="s1">])</span>
<span class="s0">#%% 
# Test your functions here - No need to change this code - just run this cell</span>
<span class="s2">assert </span><span class="s1">set(get_article_names([</span><span class="s4">1024</span><span class="s2">, </span><span class="s4">1176</span><span class="s2">, </span><span class="s4">1305</span><span class="s2">, </span><span class="s4">1314</span><span class="s2">, </span><span class="s4">1422</span><span class="s2">, </span><span class="s4">1427</span><span class="s1">])) == set([</span><span class="s3">'using deep learning to reconstruct high-resolution audio'</span><span class="s2">, </span><span class="s3">'build a python app on the streaming analytics service'</span><span class="s2">, </span><span class="s3">'gosales transactions for naive bayes model'</span><span class="s2">, </span><span class="s3">'healthcare python streaming application demo'</span><span class="s2">, </span><span class="s3">'use r dataframes &amp; ibm watson natural language understanding'</span><span class="s2">, </span><span class="s3">'use xgboost, scikit-learn &amp; ibm watson machine learning apis'</span><span class="s1">])</span><span class="s2">, </span><span class="s3">&quot;Oops! Your the get_article_names function doesn't work quite how we expect.&quot;</span>
<span class="s2">assert </span><span class="s1">set(get_article_names([</span><span class="s4">1320</span><span class="s2">, </span><span class="s4">232</span><span class="s2">, </span><span class="s4">844</span><span class="s1">])) == set([</span><span class="s3">'housing (2015): united states demographic measures'</span><span class="s2">,</span><span class="s3">'self-service data preparation with ibm data refinery'</span><span class="s2">,</span><span class="s3">'use the cloudant-spark connector in python notebook'</span><span class="s1">])</span><span class="s2">, </span><span class="s3">&quot;Oops! Your the get_article_names function doesn't work quite how we expect.&quot;</span>
<span class="s2">assert </span><span class="s1">set(get_user_articles(</span><span class="s4">20</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">]) == set([</span><span class="s4">1320</span><span class="s2">, </span><span class="s4">232</span><span class="s2">, </span><span class="s4">844</span><span class="s1">])</span>
<span class="s2">assert </span><span class="s1">set(get_user_articles(</span><span class="s4">20</span><span class="s1">)[</span><span class="s4">1</span><span class="s1">]) == set([</span><span class="s3">'housing (2015): united states demographic measures'</span><span class="s2">, </span><span class="s3">'self-service data preparation with ibm data refinery'</span><span class="s2">,</span><span class="s3">'use the cloudant-spark connector in python notebook'</span><span class="s1">])</span>
<span class="s2">assert </span><span class="s1">set(get_user_articles(</span><span class="s4">2</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">]) == set([</span><span class="s4">1024</span><span class="s2">, </span><span class="s4">1176</span><span class="s2">, </span><span class="s4">1305</span><span class="s2">, </span><span class="s4">1314</span><span class="s2">, </span><span class="s4">1422</span><span class="s2">, </span><span class="s4">1427</span><span class="s1">])</span>
<span class="s2">assert </span><span class="s1">set(get_user_articles(</span><span class="s4">2</span><span class="s1">)[</span><span class="s4">1</span><span class="s1">]) == set([</span><span class="s3">'using deep learning to reconstruct high-resolution audio'</span><span class="s2">, </span><span class="s3">'build a python app on the streaming analytics service'</span><span class="s2">, </span><span class="s3">'gosales transactions for naive bayes model'</span><span class="s2">, </span><span class="s3">'healthcare python streaming application demo'</span><span class="s2">, </span><span class="s3">'use r dataframes &amp; ibm watson natural language understanding'</span><span class="s2">, </span><span class="s3">'use xgboost, scikit-learn &amp; ibm watson machine learning apis'</span><span class="s1">])</span>
<span class="s2">assert </span><span class="s1">get_ranked_article_unique_counts([</span><span class="s4">1320</span><span class="s2">, </span><span class="s4">232</span><span class="s2">, </span><span class="s4">844</span><span class="s1">])[</span><span class="s4">0</span><span class="s1">] == [</span><span class="s4">1320</span><span class="s2">, </span><span class="s4">123</span><span class="s1">]</span><span class="s2">, </span><span class="s3">&quot;Oops! Your the get_ranked_article_unique_counts function doesn't work quite how we expect.</span><span class="s2">\n</span><span class="s3">Make sure you are using the user_item matrix to create the article counts.&quot;</span>
<span class="s1">print(</span><span class="s3">&quot;If this is all you see, you passed all of our tests!  Nice job!&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.   
 
* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions. 
 
 
* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier. 
</span><span class="s0">#%% 
</span><span class="s1">user_id = </span><span class="s4">2</span>
<span class="s1">user_ids</span><span class="s2">, </span><span class="s1">sim = find_similar_users(user_id</span><span class="s2">, </span><span class="s1">include_similarity=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s1">neighbors_df = pd.DataFrame({</span><span class="s3">'neighbor_id'</span><span class="s1">:user_ids</span><span class="s2">,</span>
                                     <span class="s3">'similarity'</span><span class="s1">:sim})</span>


<span class="s0">#%% 
</span><span class="s1">num_interactions = []</span>
<span class="s2">for </span><span class="s1">row </span><span class="s2">in </span><span class="s1">range(len(neighbors_df)):</span>
    <span class="s1">id = int(neighbors_df.iloc[row][</span><span class="s3">'neighbor_id'</span><span class="s1">])-</span><span class="s4">1 </span><span class="s0"># correct id for 0 start </span>
    <span class="s1">num_interactions.append(user_item.iloc[id].sum())</span>
    
<span class="s1">neighbors_df[</span><span class="s3">&quot;num_interactions&quot;</span><span class="s1">] = num_interactions</span>
<span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">get_top_sorted_users(user_id</span><span class="s2">, </span><span class="s1">user_item=user_item):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    user_id - (int) 
    user_item - (pandas dataframe) matrix of users by articles:  
            1's when a user has interacted with an article, 0 otherwise 
     
             
    OUTPUT: 
    neighbors_df - (pandas dataframe) a dataframe with: 
                    neighbor_id - is a neighbor user_id 
                    similarity - measure of the similarity of each user to the provided user_id 
                    num_interactions - the number of articles viewed by the user 
                     
    Other Details - sort the neighbors_df by the similarity and then by number of interactions where  
                    highest of each is higher in the dataframe, i.e. Descending order 
      
    &quot;&quot;&quot;</span>
    <span class="s1">user_ids</span><span class="s2">, </span><span class="s1">sim = find_similar_users(user_id</span><span class="s2">, </span><span class="s1">include_similarity=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">neighbors_df = pd.DataFrame({</span><span class="s3">'neighbor_id'</span><span class="s1">:user_ids</span><span class="s2">,</span>
                                     <span class="s3">'similarity'</span><span class="s1">:sim})</span>
    <span class="s1">num_interactions = []</span>
    <span class="s2">for </span><span class="s1">row </span><span class="s2">in </span><span class="s1">range(len(neighbors_df)):</span>
        <span class="s1">id = int(neighbors_df.iloc[row][</span><span class="s3">'neighbor_id'</span><span class="s1">])-</span><span class="s4">1</span>
        <span class="s1">num_interactions.append(user_item.iloc[id].sum())</span>
    
    <span class="s1">neighbors_df[</span><span class="s3">&quot;num_interactions&quot;</span><span class="s1">] = num_interactions</span>

    <span class="s1">neighbors_df.sort_values(by=[</span><span class="s3">&quot;similarity&quot;</span><span class="s2">,</span><span class="s3">'num_interactions'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">ascending=</span><span class="s2">False, </span><span class="s1">inplace=</span><span class="s2">True, </span><span class="s1">ignore_index=</span><span class="s2">True</span><span class="s1">)</span>
    
    <span class="s2">return </span><span class="s1">neighbors_df </span><span class="s0"># Return the dataframe specified in the doc_string</span>
<span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">user_user_recs_part2(user_id</span><span class="s2">, </span><span class="s1">m=</span><span class="s4">10</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    user_id - (int) a user id 
    m - (int) the number of recommendations you want for the user 
     
    OUTPUT: 
    recs - (list) a list of recommendations for the user by article id 
    rec_names - (list) a list of recommendations for the user by article title 
     
    Description: 
    Loops through the users based on closeness to the input user_id 
    For each user - finds articles the user hasn't seen before and provides them as recs 
    Does this until m recommendations are found 
     
    Notes: 
    * Choose the users that have the most total article interactions  
    before choosing those with fewer article interactions. 
 
    * Choose articles with the articles with the most total interactions  
    before choosing those with fewer total interactions.  
    
    &quot;&quot;&quot;</span>
    <span class="s1">top_sorted_users = get_top_sorted_users(user_id)</span>
    <span class="s1">recs = [] </span><span class="s0"># initalize recs</span>
    <span class="s1">user = user_item.iloc[user_id] </span><span class="s0"># initialize user</span>

    <span class="s2">for </span><span class="s1">u </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s2">,</span><span class="s1">len(top_sorted_users)):</span>
        <span class="s1">id = top_sorted_users[</span><span class="s3">&quot;neighbor_id&quot;</span><span class="s1">][u]</span>
        <span class="s1">sim_user = user_item.iloc[id]</span>
        <span class="s1">result = user_item.columns[(sim_user == </span><span class="s4">1</span><span class="s1">) &amp; (user == </span><span class="s4">0</span><span class="s1">)].tolist()</span>
            
        <span class="s1">recs.extend([item </span><span class="s2">for </span><span class="s1">item </span><span class="s2">in </span><span class="s1">result </span><span class="s2">if </span><span class="s1">item </span><span class="s2">not in </span><span class="s1">recs])</span>
            
        <span class="s0"># Check if we have enough recommendations</span>
        <span class="s1">n_recs = len(recs)</span>
        <span class="s2">if </span><span class="s1">n_recs &gt;= m:</span>
            <span class="s2">break  </span><span class="s0"># Exit loop if we have enough recommendations</span>
        
    <span class="s1">recs = recs[:m]</span>
    <span class="s1">recs = list(map(int</span><span class="s2">, </span><span class="s1">recs))</span>

    <span class="s0"># sort articles by most total interaction </span>
    <span class="s1">recs_df = df[df[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].isin(recs)]</span>
    <span class="s1">recs_df = recs_df.groupby(</span><span class="s3">&quot;article_id&quot;</span><span class="s1">)[</span><span class="s3">&quot;user_id&quot;</span><span class="s1">].count().reset_index()</span>
    <span class="s1">recs_df.columns = [</span><span class="s3">&quot;article_id&quot;</span><span class="s2">, </span><span class="s3">&quot;count&quot;</span><span class="s1">]</span>
    <span class="s1">recs_df = recs_df.sort_values(by=</span><span class="s3">&quot;count&quot;</span><span class="s2">, </span><span class="s1">ascending=</span><span class="s2">False</span><span class="s1">).reset_index(drop=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">recs = recs_df[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].tolist()</span>
    
    <span class="s2">return </span><span class="s1">recs</span><span class="s2">, </span><span class="s1">get_article_names(recs) </span><span class="s0"># return your recommendations for this user_id</span>
<span class="s0">#%% 
# Quick spot check - don't change this code - just use it to test your functions</span>
<span class="s1">rec_ids</span><span class="s2">, </span><span class="s1">rec_names = user_user_recs_part2(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">&quot;The top 10 recommendations for user 20 are the following article ids:&quot;</span><span class="s1">)</span>
<span class="s1">print(rec_ids)</span>
<span class="s1">print()</span>
<span class="s1">print(</span><span class="s3">&quot;The top 10 recommendations for user 20 are the following article names:&quot;</span><span class="s1">)</span>
<span class="s1">print(rec_names)</span>
<span class="s0">#%% md 
</span><span class="s1">`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below. 
</span><span class="s0">#%% 
</span><span class="s1">print(get_top_sorted_users(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">user_item=user_item).head(n=</span><span class="s4">1</span><span class="s1">))</span>
<span class="s1">print(get_top_sorted_users(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">user_item=user_item).head(n=</span><span class="s4">10</span><span class="s1">))</span>
<span class="s1">print(get_top_sorted_users(</span><span class="s4">131</span><span class="s2">, </span><span class="s1">user_item=user_item).head(n=</span><span class="s4">10</span><span class="s1">))</span>
<span class="s0">#%% 
### Tests with a dictionary of results</span>
<span class="s1">user1_most_sim =  get_top_sorted_users(</span><span class="s4">1</span><span class="s1">)[</span><span class="s3">&quot;neighbor_id&quot;</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0"># Find the user that is most similar to user 1 </span>
<span class="s1">user2_6th_sim =  get_top_sorted_users(</span><span class="s4">2</span><span class="s1">)[</span><span class="s3">&quot;neighbor_id&quot;</span><span class="s1">][</span><span class="s4">5</span><span class="s1">]</span><span class="s0"># Find the 6th most similar user to user 2</span>
<span class="s1">user131_10th_sim =  get_top_sorted_users(</span><span class="s4">131</span><span class="s1">)[</span><span class="s3">&quot;neighbor_id&quot;</span><span class="s1">][</span><span class="s4">9</span><span class="s1">]</span><span class="s0"># Find the 10th most similar user to user 131</span>
<span class="s0">#%% 
## Dictionary Test Here</span>
<span class="s1">sol_5_dict = {</span>
    <span class="s3">'The user that is most similar to user 1.'</span><span class="s1">: user1_most_sim</span><span class="s2">, </span>
    <span class="s3">'The user that is the 6th most similar to user 2.'</span><span class="s1">: user2_6th_sim</span><span class="s2">,</span>
    <span class="s3">'The user that is the 10th most similar to user 131.'</span><span class="s1">: user131_10th_sim</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s1">t.sol_5_test(sol_5_dict)</span>
<span class="s0">#%% md 
</span><span class="s1">`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users. 
</span><span class="s0">#%% md 
</span><span class="s1">Answer: 
 
If no user history is available, we can not use a recommendation based on user similarity. Therefore, we could only recommend articles based on how many interactions they have: *get_top_articles* 
</span><span class="s0">#%% md 
</span><span class="s1">`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation. 
</span><span class="s0">#%% 
# What would your recommendations be for this new user 0?  As a new user, they have no observed articles.</span>
<span class="s0"># Provide a list of the top 10 article ids you would give to </span>
<span class="s1">new_user_recs = get_top_article_ids(</span><span class="s4">10</span><span class="s1">)</span>

<span class="s0">#%% 
</span><span class="s2">assert </span><span class="s1">set(new_user_recs) == {</span><span class="s4">1314</span><span class="s2">, </span><span class="s4">1429</span><span class="s2">, </span><span class="s4">1293</span><span class="s2">, </span><span class="s4">1427</span><span class="s2">, </span><span class="s4">1162</span><span class="s2">, </span><span class="s4">1364</span><span class="s2">, </span><span class="s4">1304</span><span class="s2">, </span><span class="s4">1170</span><span class="s2">, </span><span class="s4">1431</span><span class="s2">, </span><span class="s4">1330</span><span class="s1">}</span><span class="s2">, </span><span class="s3">&quot;Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.&quot;</span>

<span class="s1">print(</span><span class="s3">&quot;That's right!  Nice job!&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">### &lt;a class=&quot;anchor&quot; id=&quot;Content-Recs&quot;&gt;Part IV: Content Based Recommendations&lt;/a&gt; 
 
Another method we might use to make recommendations is to recommend similar articles that are possibly related. One way we can find article relationships is by clustering text about those articles.  Let's consider content to be the article **title**, as it is the only text we have available. One point to highlight, there isn't one way to create a content based recommendation, especially considering that text information can be processed in many ways.   
 
`1.` Use the function bodies below to create a content based recommender function `make_content_recs`. We'll use TF-IDF to create a matrix based off article titles, and use this matrix to create clusters of related articles. You can use this function to make recommendations of new articles. 
</span><span class="s0">#%% 
</span><span class="s1">df.head()</span>
<span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span>
<span class="s2">from </span><span class="s1">sklearn.feature_extraction.text </span><span class="s2">import </span><span class="s1">TfidfVectorizer</span>
<span class="s2">from </span><span class="s1">sklearn.pipeline </span><span class="s2">import </span><span class="s1">make_pipeline</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">Normalizer</span>
<span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">TruncatedSVD</span>
<span class="s0">#%% 
# unique articles</span>
<span class="s1">df_unique_articles = df.drop_duplicates(subset=[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">keep=</span><span class="s3">&quot;first&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
# Create a vectorizer using TfidfVectorizer and fit it to the article titles</span>
<span class="s1">max_features = </span><span class="s4">200</span>
<span class="s1">max_df = </span><span class="s4">0.75</span>
<span class="s1">min_df = </span><span class="s4">5</span>

<span class="s1">vectorizer = TfidfVectorizer(</span>
    <span class="s1">max_df=max_df</span><span class="s2">,</span>
    <span class="s1">min_df=min_df</span><span class="s2">,</span>
    <span class="s1">stop_words=</span><span class="s3">&quot;english&quot;</span><span class="s2">,</span>
    <span class="s1">max_features=max_features</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">print(</span><span class="s3">&quot;Running TF-IDF&quot;</span><span class="s1">)</span>
<span class="s1">X_tfidf = vectorizer.fit_transform(df_unique_articles[</span><span class="s3">&quot;title&quot;</span><span class="s1">]) </span><span class="s0"># Fit the vectorizer to the article titles</span>

<span class="s1">print(</span><span class="s3">f&quot;n_samples: </span><span class="s2">{</span><span class="s1">X_tfidf.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">}</span><span class="s3">, n_features: </span><span class="s2">{</span><span class="s1">X_tfidf.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

<span class="s1">lsa = make_pipeline(TruncatedSVD(n_components=</span><span class="s4">50</span><span class="s1">)</span><span class="s2">, </span><span class="s1">Normalizer(copy=</span><span class="s2">False</span><span class="s1">))</span>
<span class="s1">X_lsa = lsa.fit_transform(X_tfidf)</span><span class="s0"># Fit the LSA model to the vectorized article titles</span>
<span class="s1">explained_variance = lsa[</span><span class="s4">0</span><span class="s1">].explained_variance_ratio_.sum()</span>

<span class="s1">print(</span><span class="s3">f&quot;Explained variance of the SVD step: </span><span class="s2">{</span><span class="s1">explained_variance * </span><span class="s4">100</span><span class="s2">:</span><span class="s3">.1f</span><span class="s2">}</span><span class="s3">%&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
# Let's map the inertia for different number of clusters to find the optimal number of clusters</span>
<span class="s0"># We'll plot it to see the elbow</span>
<span class="s1">inertia = []</span>
<span class="s1">clusters = </span><span class="s4">300</span>
<span class="s1">step = </span><span class="s4">25</span>
<span class="s1">max_iter = </span><span class="s4">50</span>
<span class="s1">n_init = </span><span class="s4">5</span>
<span class="s1">random_state = </span><span class="s4">42</span>
<span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">clusters</span><span class="s2">, </span><span class="s1">step):</span>
    <span class="s1">kmeans = KMeans(</span>
        <span class="s1">n_clusters=k</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">n_init=n_init</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
    <span class="s1">).fit(X_lsa)</span>
    <span class="s0"># inertia is the sum of squared distances to the closest cluster center</span>
    <span class="s1">inertia.append(kmeans.inertia_)</span>
<span class="s1">plt.plot(range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">clusters</span><span class="s2">, </span><span class="s1">step)</span><span class="s2">, </span><span class="s1">inertia)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Number of clusters'</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">There appears to be an elbow about 50, so we'll use 50 clusters. 
</span><span class="s0">#%% 
</span><span class="s1">n_clusters = </span><span class="s4">50</span>
<span class="s1">kmeans = KMeans(</span>
        <span class="s1">n_clusters=n_clusters</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">n_init=n_init</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
<span class="s1">).fit(X_lsa)</span>
<span class="s0">#%% 
# create a new column `title_cluster` and assign it the kmeans cluster labels</span>
<span class="s0"># First we need to map the labels to df_unique_articles article ids and then apply those to df</span>

<span class="s1">pred_cluster = kmeans.predict(X_lsa)</span>
<span class="s1">article_cluster_map = dict(zip(df_unique_articles[</span><span class="s3">'article_id'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">pred_cluster))</span>

<span class="s1">df[</span><span class="s3">'title_cluster'</span><span class="s1">] = df[</span><span class="s3">'article_id'</span><span class="s1">].map(article_cluster_map)</span><span class="s0"># apply map to create title clusters</span>
<span class="s0">#%% 
# Let's check the number of articles in each cluster</span>
<span class="s1">np.array(np.unique(kmeans.labels_</span><span class="s2">, </span><span class="s1">return_counts=</span><span class="s2">True</span><span class="s1">)).T</span>
<span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">get_similar_articles(article_id</span><span class="s2">, </span><span class="s1">df=df):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    article_id - (int) an article id  
    df - (pandas dataframe) df as defined at the top of the notebook  
     
    OUTPUT: 
    article_ids - (list) a list of article ids that are in the same title cluster 
     
    Description: 
    Returns a list of the article ids that are in the same title cluster 
    &quot;&quot;&quot;</span>
     <span class="s0"># Your code here</span>
    <span class="s1">title_cluster = df.loc[df[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">] == article_id</span><span class="s2">, </span><span class="s3">&quot;title_cluster&quot;</span><span class="s1">].values[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">articles_in_cluster = df[df[</span><span class="s3">&quot;title_cluster&quot;</span><span class="s1">]==title_cluster]</span>

    <span class="s0"># remove the input article_id from the list</span>
    <span class="s1">articles_in_cluster = articles_in_cluster[articles_in_cluster[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">] != article_id]</span>
        
    <span class="s1">articles_in_cluster = articles_in_cluster[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].unique().tolist()</span>
    
    <span class="s2">return </span><span class="s1">articles_in_cluster</span>
<span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">make_content_recs(article_id</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">df=df):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    article_id - (int) an article id 
    n - (int) the number of recommendations you want similar to the article id 
    df - (pandas dataframe) df as defined at the top of the notebook 
     
    OUTPUT: 
    n_ranked_similar_articles - (list) a list of article ids that are in the same title cluster ranked 
                                by popularity 
    n_ranked_article_names - (list) a list of article names associated with the list of article ids 
     
    Description: 
    Returns a list of the n most ranked similar articles to a given article_id based on the title 
    cluster in df. Rank similar articles using the function get_ranked_article_unique_counts. 
    &quot;&quot;&quot;</span>
    <span class="s1">similar_articles = get_similar_articles(article_id)</span>
    
    <span class="s1">n_ranked_similar_articles = get_ranked_article_unique_counts(similar_articles)[:n]</span>
    <span class="s1">n_ranked_similar_articles = [sublist[</span><span class="s4">0</span><span class="s1">] </span><span class="s2">for </span><span class="s1">sublist </span><span class="s2">in </span><span class="s1">n_ranked_similar_articles]</span>
    
    <span class="s1">n_ranked_article_names = get_article_names(n_ranked_similar_articles)</span>
    
    <span class="s2">return </span><span class="s1">n_ranked_similar_articles</span><span class="s2">, </span><span class="s1">n_ranked_article_names</span>
        
<span class="s0">#%% 
# Test out your content recommendations given artice_id 25</span>
<span class="s1">rec_article_ids</span><span class="s2">, </span><span class="s1">rec_article_titles = make_content_recs(</span><span class="s4">25</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
<span class="s1">print(rec_article_ids)</span>
<span class="s1">print(rec_article_titles)</span>
<span class="s0">#%% 
</span><span class="s2">assert </span><span class="s1">len({</span><span class="s4">1025</span><span class="s2">, </span><span class="s4">593</span><span class="s2">, </span><span class="s4">349</span><span class="s2">, </span><span class="s4">821</span><span class="s2">, </span><span class="s4">464</span><span class="s2">, </span><span class="s4">29</span><span class="s2">, </span><span class="s4">1042</span><span class="s2">, </span><span class="s4">693</span><span class="s2">, </span><span class="s4">524</span><span class="s2">, </span><span class="s4">352</span><span class="s1">}.intersection(set(rec_article_ids))) &gt; </span><span class="s4">0</span><span class="s2">, </span><span class="s3">&quot;Oops! Your the make_content_recs function doesn't work quite how we expect.&quot;</span>
<span class="s0">#%% md 
</span><span class="s1">`2.` Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function? What other text data would be useful to help make better recommendations besides the article title? 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
 
The content-based recommendation system works by extracting information from article titles using natural language processing (TF-IDF). A vocabulary of 125 terms is extracted and used to cluster the titles into 50 distinct clusters. The number of clusters was determined by analyzing the elbow point of the scree plot.  
 
When recommending articles, those that belong to the same cluster as the provided article are suggested, with higher-ranked articles being prioritized.  
 
To improve the content-based recommendation system, it would be beneficial to include additional information beyond just the title. Specifically, the article text data would provide deeper insights into the full article content. Additionally, metadata such as the publication date and author could further enhance the clustering process, by including factors like recency and authorship, allowing the system to make more relevant and timely recommendations. 
 
</span><span class="s0">#%% md 
</span><span class="s1">### &lt;a class=&quot;anchor&quot; id=&quot;Matrix-Fact&quot;&gt;Part V: Matrix Factorization&lt;/a&gt; 
 
In this part of the notebook, you will build use matrix factorization to make article recommendations to users. 
 
`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook.  
</span><span class="s0">#%% 
# quick look at the matrix</span>
<span class="s1">user_item.head()</span>
<span class="s0">#%% md 
</span><span class="s1">`2.` In this situation, you can use Singular Value Decomposition from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) on the user-item matrix.  Use the cell to perform SVD. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">TruncatedSVD</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">precision_score</span><span class="s2">, </span><span class="s1">recall_score</span><span class="s2">, </span><span class="s1">accuracy_score</span>
<span class="s0"># Using the full number of components which equals the number of columns</span>
<span class="s1">svd = TruncatedSVD(n_components=len(user_item.columns)</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">42</span><span class="s1">)</span>

<span class="s1">u = svd.fit_transform(user_item)</span>
<span class="s1">v = svd.components_</span>
<span class="s1">s = svd.singular_values_ </span>
<span class="s1">print(</span><span class="s3">'u'</span><span class="s2">, </span><span class="s1">u.shape)</span>
<span class="s1">print(</span><span class="s3">'s'</span><span class="s2">, </span><span class="s1">s.shape)</span>
<span class="s1">print(</span><span class="s3">'vt'</span><span class="s2">, </span><span class="s1">v.shape)</span>
<span class="s0">#%% md 
</span><span class="s1">`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain better metrics when making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how our metrics improve as we increase the number of latent features. 
</span><span class="s0">#%% md 
</span>
<span class="s0">#%% 
</span><span class="s1">num_latent_feats = np.arange(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">700</span><span class="s1">+</span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s1">)</span>
<span class="s1">metric_scores = []</span>

<span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">num_latent_feats:</span>
    <span class="s0"># restructure with k latent features</span>
    <span class="s1">u_new</span><span class="s2">, </span><span class="s1">vt_new = u[:</span><span class="s2">, </span><span class="s1">:k]</span><span class="s2">, </span><span class="s1">v[:k</span><span class="s2">, </span><span class="s1">:]</span>
    
    <span class="s0"># take dot product</span>
    <span class="s1">user_item_est = abs(np.around(np.dot(u_new</span><span class="s2">, </span><span class="s1">vt_new))).astype(int)</span>
    <span class="s0"># make sure the values are between 0 and 1</span>
    <span class="s1">user_item_est = np.clip(user_item_est</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    
    <span class="s0"># total errors and keep track of them</span>
    <span class="s1">acc = accuracy_score(user_item.values.flatten()</span><span class="s2">, </span><span class="s1">user_item_est.flatten())</span>
    <span class="s1">precision = precision_score(user_item.values.flatten()</span><span class="s2">, </span><span class="s1">user_item_est.flatten())</span>
    <span class="s1">recall = recall_score(user_item.values.flatten()</span><span class="s2">, </span><span class="s1">user_item_est.flatten())</span>
    <span class="s1">metric_scores.append([acc</span><span class="s2">, </span><span class="s1">precision</span><span class="s2">, </span><span class="s1">recall])</span>
    
    
<span class="s1">plt.plot(num_latent_feats</span><span class="s2">, </span><span class="s1">metric_scores</span><span class="s2">, </span><span class="s1">label=[</span><span class="s3">'Accuracy'</span><span class="s2">, </span><span class="s3">'Precision'</span><span class="s2">, </span><span class="s3">'Recall'</span><span class="s1">])</span>
<span class="s1">plt.legend()</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Number of Latent Features'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Metrics vs. Number of Latent Features'</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations. Given the plot above, what would you pick for the number of latent features and why? 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
 
I would select the number of latent features at the elbow point which occurs around 200 latent features. While increasing the number of latent features improves recall, it could also make the model overly complex. It is important to find a balance between computational complexity and model performance, especially since the improvements stagnate after the elbow point. 
</span><span class="s0">#%% md 
</span>
<span class="s0">#%% md 
</span><span class="s1">`5.` Using 200 latent features and the values of U, S, and V transpose we calculated above, create an article id recommendation function that finds similar article ids to the one provide. 
 
Create a list of 10 recommendations that are similar to article with id 4.  The function should provide these recommendations by finding articles that have the most similar latent features as the provided article. 
</span><span class="s0">#%% 
</span><span class="s1">article_id = </span><span class="s4">100</span>
<span class="s1">article_id = str(article_id)</span>
<span class="s1">article_idx= user_item.columns.tolist().index(article_id)</span>
<span class="s1">vt = v</span>
<span class="s1">cos_sim = cosine_similarity(vt.T)</span>

<span class="s1">article_sim = pd.DataFrame({</span><span class="s3">&quot;sim&quot;</span><span class="s1">: cos_sim[article_idx]</span><span class="s2">,</span>
                            <span class="s3">&quot;article_id&quot;</span><span class="s1">: user_item.columns.values</span>
                            <span class="s1">})</span>
<span class="s1">article_sim = article_sim.drop(article_idx)</span>

<span class="s1">article_sim.sort_values(by=</span><span class="s3">&quot;sim&quot;</span><span class="s2">, </span><span class="s1">ascending=</span><span class="s2">False, </span><span class="s1">inplace=</span><span class="s2">True, </span><span class="s1">ignore_index=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s1">similar_article_id = article_sim[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">]</span>
<span class="s1">similarity = article_sim[</span><span class="s3">&quot;sim&quot;</span><span class="s1">]</span>

<span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">get_svd_similar_article_ids(article_id</span><span class="s2">, </span><span class="s1">vt</span><span class="s2">, </span><span class="s1">user_item=user_item</span><span class="s2">, </span><span class="s1">include_similarity=</span><span class="s2">False</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot; 
    INPUT: 
    article_id - (int) an article id 
    vt - (numpy array) vt matrix from SVD 
    user_item - (pandas dataframe) matrix of users by articles:  
                1's when a user has interacted with an article, 0 otherwise 
    include_similarity - (bool) whether to include the similarity in the output 
     
    OUTPUT: 
    article_ids - (list) a list of article ids that are in the same title cluster 
     
    Description: 
    Returns a list of the article ids similar using SVD factorization 
    &quot;&quot;&quot;</span>
    <span class="s0"># Find the index of the article_id</span>
    <span class="s1">article_id = str(article_id)</span>
    <span class="s1">article_idx= user_item.columns.tolist().index(article_id)</span>
    <span class="s0"># Find the cosine similarity of all articles</span>
    <span class="s1">cos_sim = cosine_similarity(vt.T)</span>
    <span class="s0"># Get similarities only for the cos_sim of the article_idx</span>
    <span class="s1">article_sim = pd.DataFrame({</span><span class="s3">&quot;sim&quot;</span><span class="s1">: cos_sim[article_idx]</span><span class="s2">,</span>
                            <span class="s3">&quot;article_id&quot;</span><span class="s1">: user_item.columns.values</span>
                            <span class="s1">})</span>
    
    <span class="s1">article_sim = article_sim.drop(article_idx)</span>
    <span class="s0"># Sort and return the articles, don't include the own article</span>
    <span class="s1">article_sim.sort_values(by=</span><span class="s3">&quot;sim&quot;</span><span class="s2">, </span><span class="s1">ascending=</span><span class="s2">False, </span><span class="s1">inplace=</span><span class="s2">True, </span><span class="s1">ignore_index=</span><span class="s2">True</span><span class="s1">)</span>
     
    <span class="s1">similar_article_id = article_sim[</span><span class="s3">&quot;article_id&quot;</span><span class="s1">].astype(int)</span>
    <span class="s1">similarity = article_sim[</span><span class="s3">&quot;sim&quot;</span><span class="s1">]</span>
    
    <span class="s2">if </span><span class="s1">include_similarity:</span>
        <span class="s2">return </span><span class="s1">[[similar_article_id</span><span class="s2">, </span><span class="s1">similarity]]</span>
    <span class="s2">return </span><span class="s1">similar_article_id.tolist()</span>
<span class="s0">#%% 
# Create a vt_new matrix with 200 latent features</span>
<span class="s1">k = </span><span class="s4">200</span>
<span class="s1">vt_new = v[:k</span><span class="s2">, </span><span class="s1">:]</span>
<span class="s0">#%% 
# What is the article name for article_id 4?</span>
<span class="s1">print(</span><span class="s3">&quot;Current article:&quot;</span><span class="s2">, </span><span class="s1">get_article_names([</span><span class="s4">4</span><span class="s1">]</span><span class="s2">, </span><span class="s1">df=df)[</span><span class="s4">0</span><span class="s1">])</span>
<span class="s0">#%% 
# What are the top 10 most similar articles to article_id 4?</span>
<span class="s1">rec_articles = get_svd_similar_article_ids(</span><span class="s4">4</span><span class="s2">, </span><span class="s1">vt_new</span><span class="s2">, </span><span class="s1">user_item=user_item)[:</span><span class="s4">10</span><span class="s1">]</span>
<span class="s1">rec_articles</span>
<span class="s0">#%% 
# What are the top 10 most similar articles to article_id 4?</span>
<span class="s1">get_article_names(rec_articles</span><span class="s2">, </span><span class="s1">df=df)</span>
<span class="s0">#%% 
</span><span class="s2">assert </span><span class="s1">set(rec_articles) == {</span><span class="s4">1199</span><span class="s2">, </span><span class="s4">1068</span><span class="s2">, </span><span class="s4">486</span><span class="s2">, </span><span class="s4">1202</span><span class="s2">, </span><span class="s4">176</span><span class="s2">, </span><span class="s4">1120</span><span class="s2">, </span><span class="s4">244</span><span class="s2">, </span><span class="s4">793</span><span class="s2">, </span><span class="s4">58</span><span class="s2">, </span><span class="s4">132</span><span class="s1">}</span><span class="s2">, </span><span class="s3">&quot;Oops! Your the get_svd_similar_article_ids function doesn't work quite how we expect.&quot;</span>
<span class="s1">print(</span><span class="s3">&quot;That's right!  Great job!&quot;</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">make_content_recs(article_id=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">n=</span><span class="s4">10</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make above are an improvement to how users currently find articles, either by Sections 2, 3, or 4? Add any tradeoffs between each of the methods, and how you could leverage each type for different situations including new users with no history, recently new users with little history, and users with a lot of history.  
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
 
Each recommendation model has its strengths and limitations. While rank-based models do not provide personalized recommendations, they can work well for new users without prior history. For example, they are useful for a homepage showcasing the most frequently interacted-with articles. However, this approach tends to favor popular articles, making it less effective for newer or niche articles. 
 
User-user based collaborative filtering provides personalized recommendations based on similar users' preferences. This makes it ideal for sections that suggest articles liked by others with similar interests. However, since it does not consider article content, recommendations may not always be thematically relevant. Additionally, this method required user history and is computationally intensive due to pairwise similarity calculations. 
 
In contrast, content-based recommendations suggest articles based on their content. This approach is useful for recommending articles closely related to the one a user is currently reading. It also works well for new users, as it does not rely on user history. However, it may limit diversity, as it tends to recommend articles within the same topic range.  
 
Matrix-factorization provide recommendations based on hidden latent factors. This model is effective with sparse data and can, therefore, be used in situations where only little user history is available. However, this model lacks interpretability, making it difficult to explain why a particular article was recommended and what the hidden latent factors represent.  
 
To evaluate the effectivenss of these models, A/B testing can be implemented. Key metrics such as conversion rate, click-trough rate, and user engagement can help determine performance. Additionally, interleaved testing (presenting two recommendation models simultaneously) can be used to directly compare two models and evaluate which recommendations where preferred.  
 
Finally, since each model produces different recommendations, combining them may improve overall performance. For example, content-based recommendations for *article_id 4* focus primarily on the analytics aspect of the article title, while matrix factorization suggests a more diverse set of articles. A hybrid approach, blending content-based filtering for relevance with matrix factorization for diversity, could enhance recommendation quality by balancing specificity with broader exploration. The hybrid approach would of course need to be properly tested.  
 
</span><span class="s0">#%% 
#from subprocess import call</span>
<span class="s0">#call(['python', '-m', 'nbconvert', '--to html', 'Recommendations_with_IBM.ipynb'])</span></pre>
</body>
</html>